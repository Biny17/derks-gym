{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 21:45:34.894350: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from src import environments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from gym_derk.envs import DerkEnv\n",
    "import gym_derk\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from src.environments import EnvPerso, EnvPersoInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward= {\n",
    "    \"damageEnemyStatue\": 4,\n",
    "    \"damageEnemyUnit\": 2,\n",
    "    \"killEnemyStatue\": 4,\n",
    "    \"killEnemyUnit\": 2,\n",
    "    \"healFriendlyStatue\": 1,\n",
    "    \"healTeammatel\": 2,\n",
    "    \"healTeammate2\": 2,\n",
    "    \"timeSpentHomeBase\": 0,\n",
    "    \"timeSpentHomeTerritory\": 0,\n",
    "    \"timeSpentAwayTerritory\": 0,\n",
    "    \"timeSpentAwayBase\": 0,\n",
    "    \"damageTaken\": -1,\n",
    "    \"friendlyFire\": -1,\n",
    "    \"healEnemy\": -1,\n",
    "    \"fallDamageTaken\": -10,\n",
    "    \"statueDamageTaken\": 0,\n",
    "    \"manualBonus\": 0,\n",
    "    \"victory\": 100,\n",
    "    \"loss\": -100,\n",
    "    \"tie\": 0,\n",
    "    \"teamSpirit\": 0.5,\n",
    "    \"timeScaling\": 1,\n",
    "}\n",
    "\n",
    "env = EnvPersoInput(reward_function = reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e+00,  1.00000000e+00,  2.44674265e-01, -8.38051915e-01,\n",
       "        1.90240473e-01, -2.68619299e-01,  2.56791979e-01, -3.91661733e-01,\n",
       "        1.00000000e+00, -3.68232578e-02,  8.73027384e-01, -9.80446339e-02,\n",
       "        6.61070168e-01, -6.10202625e-02,  7.15698183e-01,  7.76933972e-04,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.11463346e-01,  1.11463346e-01,  2.02826411e-01,\n",
       "       -3.90740000e-02, -1.33064091e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(15,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected observation and reward specs to both be either tensor or array specs, but saw spec values BoundedTensorSpec(shape=(64,), dtype=tf.float32, name='observation', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32)) vs. ArraySpec(shape=(), dtype=dtype('float32'), name='reward')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/trist/code/Biny17/derks-gym/selection.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m train_step_counter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Create the PPO agent.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m agent \u001b[39m=\u001b[39m ppo_agent\u001b[39m.\u001b[39mPPOAgent(\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     env\u001b[39m.\u001b[39;49mtime_step_spec(),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     env\u001b[39m.\u001b[39maction_spec(),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     actor_net\u001b[39m=\u001b[39mactor_net,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     train_step_counter\u001b[39m=\u001b[39mtrain_step_counter)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Initialize the agent.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m agent\u001b[39m.\u001b[39minitialize()\n",
      "File \u001b[0;32m~/code/Biny17/derks-gym/src/environments.py:71\u001b[0m, in \u001b[0;36mEnvPerso.time_step_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtime_step_spec\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m ts\u001b[39m.\u001b[39;49mtime_step_spec(observation_spec\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_spec(), reward_spec\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_spec())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tf_agents/trajectories/time_step.py:403\u001b[0m, in \u001b[0;36mtime_step_spec\u001b[0;34m(observation_spec, reward_spec)\u001b[0m\n\u001b[1;32m    400\u001b[0m   first_reward_spec \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(reward_spec)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    401\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(first_reward_spec, tf\u001b[39m.\u001b[39mTypeSpec)\n\u001b[1;32m    402\u001b[0m       \u001b[39m!=\u001b[39m \u001b[39misinstance\u001b[39m(first_observation_spec, tf\u001b[39m.\u001b[39mTypeSpec)):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    404\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected observation and reward specs to both be either tensor or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    405\u001b[0m         \u001b[39m'\u001b[39m\u001b[39marray specs, but saw spec values \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m vs. \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[39m.\u001b[39mformat(first_observation_spec, first_reward_spec))\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(first_observation_spec, tf\u001b[39m.\u001b[39mTypeSpec):\n\u001b[1;32m    408\u001b[0m   \u001b[39mreturn\u001b[39;00m TimeStep(\n\u001b[1;32m    409\u001b[0m       step_type\u001b[39m=\u001b[39mtensor_spec\u001b[39m.\u001b[39mTensorSpec([], tf\u001b[39m.\u001b[39mint32, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstep_type\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m    410\u001b[0m       reward\u001b[39m=\u001b[39mreward_spec \u001b[39mor\u001b[39;00m tf\u001b[39m.\u001b[39mTensorSpec([], tf\u001b[39m.\u001b[39mfloat32, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m    411\u001b[0m       discount\u001b[39m=\u001b[39mtensor_spec\u001b[39m.\u001b[39mBoundedTensorSpec(\n\u001b[1;32m    412\u001b[0m           [], tf\u001b[39m.\u001b[39mfloat32, minimum\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, maximum\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdiscount\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m    413\u001b[0m       observation\u001b[39m=\u001b[39mobservation_spec)\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected observation and reward specs to both be either tensor or array specs, but saw spec values BoundedTensorSpec(shape=(64,), dtype=tf.float32, name='observation', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32)) vs. ArraySpec(shape=(), dtype=dtype('float32'), name='reward')"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Define the network for the agent.\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec())\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# Define the train step counter.\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "# Create the PPO agent.\n",
    "agent = ppo_agent.PPOAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    optimizer,\n",
    "    actor_net=actor_net,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "# Initialize the agent.\n",
    "agent.initialize()\n",
    "\n",
    "# Now you can use this agent to interact with the environment and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Agent:\n",
    "#     def __init__(self, shape_of_model: tuple):\n",
    "#         nb_outputs = 15\n",
    "#         self.model = keras.models.Sequential()\n",
    "#         self.model.add(layers.Input(shape=(64,)))\n",
    "#         for n in shape_of_model:\n",
    "#             self.model.add(layers.Dense(units=n, activation='relu'))\n",
    "#         self.model.add(layers.Dense(units=nb_outputs, activation='linear'))\n",
    "    \n",
    "#     def action(self, state: np.ndarray):\n",
    "#         prediction = self.model.predict(state)\n",
    "#         # min max scaling\n",
    "        \n",
    "#         return self.model.predict(state)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AgentManager:\n",
    "#     def __init__(self, env, number_of_agents):\n",
    "#         self.env = env\n",
    "#         self.agents = [Agent(shape_of_model=(15,15)) for i in range(number_of_agents)]\n",
    "            \n",
    "#     def take_action(self, observation):\n",
    "#         return [agent.action(observation) for agent in self.agents]   \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
