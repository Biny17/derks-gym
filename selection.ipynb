{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 21:07:30.674407: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from src import environments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from gym_derk.envs import DerkEnv\n",
    "import gym_derk\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from src.environments import EnvPerso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward= {\n",
    "    \"damageEnemyStatue\": 4,\n",
    "    \"damageEnemyUnit\": 2,\n",
    "    \"killEnemyStatue\": 4,\n",
    "    \"killEnemyUnit\": 2,\n",
    "    \"healFriendlyStatue\": 1,\n",
    "    \"healTeammatel\": 2,\n",
    "    \"healTeammate2\": 2,\n",
    "    \"timeSpentHomeBase\": 0,\n",
    "    \"timeSpentHomeTerritory\": 0,\n",
    "    \"timeSpentAwayTerritory\": 0,\n",
    "    \"timeSpentAwayBase\": 0,\n",
    "    \"damageTaken\": -1,\n",
    "    \"friendlyFire\": -1,\n",
    "    \"healEnemy\": -1,\n",
    "    \"fallDamageTaken\": -10,\n",
    "    \"statueDamageTaken\": 0,\n",
    "    \"manualBonus\": 0,\n",
    "    \"victory\": 100,\n",
    "    \"loss\": -100,\n",
    "    \"tie\": 0,\n",
    "    \"teamSpirit\": 0.5,\n",
    "    \"timeScaling\": 1,\n",
    "}\n",
    "\n",
    "env = EnvPerso(reward_function = reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EnvPerso' object has no attribute 'action_spec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/trist/code/Biny17/derks-gym/selection.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m common\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Define the network for the agent.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m actor_net \u001b[39m=\u001b[39m actor_distribution_network\u001b[39m.\u001b[39mActorDistributionNetwork(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     env\u001b[39m.\u001b[39mobservation_spec(),\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     env\u001b[39m.\u001b[39;49maction_spec(),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     fc_layer_params\u001b[39m=\u001b[39m(\u001b[39m200\u001b[39m,))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Define the optimizer.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/trist/code/Biny17/derks-gym/selection.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mAdamOptimizer(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EnvPerso' object has no attribute 'action_spec'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Define the network for the agent.\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec(),\n",
    "    fc_layer_params=(200,))\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# Define the train step counter.\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "# Create the PPO agent.\n",
    "agent = ppo_agent.PPOAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    optimizer,\n",
    "    actor_net=actor_net,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "# Initialize the agent.\n",
    "agent.initialize()\n",
    "\n",
    "# Now you can use this agent to interact with the environment and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Agent:\n",
    "#     def __init__(self, shape_of_model: tuple):\n",
    "#         nb_outputs = 15\n",
    "#         self.model = keras.models.Sequential()\n",
    "#         self.model.add(layers.Input(shape=(64,)))\n",
    "#         for n in shape_of_model:\n",
    "#             self.model.add(layers.Dense(units=n, activation='relu'))\n",
    "#         self.model.add(layers.Dense(units=nb_outputs, activation='linear'))\n",
    "    \n",
    "#     def action(self, state: np.ndarray):\n",
    "#         prediction = self.model.predict(state)\n",
    "#         # min max scaling\n",
    "        \n",
    "#         return self.model.predict(state)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AgentManager:\n",
    "#     def __init__(self, env, number_of_agents):\n",
    "#         self.env = env\n",
    "#         self.agents = [Agent(shape_of_model=(15,15)) for i in range(number_of_agents)]\n",
    "            \n",
    "#     def take_action(self, observation):\n",
    "#         return [agent.action(observation) for agent in self.agents]   \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
